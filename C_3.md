# C-3: Advanced Policy Optimization

1. Proximal Policy Optimization (PPO)
    - Clipped Surrogate Objective
    - Trust Region Optimization
    - Policy Update Clipping
    - The PPO Algorithm
2. Actor-Critic Methods
    - Actor-Critic Framework
    - Advantage Estimation
    - Bias-Variance Trade-off
    - Monte Carlo vs TD Learning
3. Asynchronous Advantage Actor-Critic (A3C)
    - Parallel Learning
    - On-policy vs Off-policy Learning
    - A2C: Synchronous Alternative
    - Generalized Advantage Estimation (GAE)

#### 1. Proximal Policy Optimization (PPO)

Proximal Policy Optimization (PPO) represents a major advancement in policy gradient methods by addressing the stability
and sample efficiency challenges of earlier approaches like REINFORCE. It balances the benefits of trust region methods
while maintaining algorithmic simplicity.

##### Clipped Surrogate Objective

The core innovation in PPO is its clipped surrogate objective function:

$$L^{\text{CLIP}}(\theta) = \mathbb{E}_t[\min(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)]$$

Where:

- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ is the probability ratio between new
  and old policies
- $A_t$ is the advantage estimate
- $\epsilon$ is a small hyperparameter (typically 0.1 or 0.2)

<div align="center"> <img src="images/clipped-surrogate.png" width="600" height="auto"> <p style="color: #555;">Figure: Visualization of the clipped surrogate objective function</p> </div>

The figure illustrates how the clipping mechanism works. The red curve shows the original surrogate function, which can
lead to excessively large policy updates. The blue curve shows the clipped version, which flattens the objective when
the policy change would be too large, effectively preventing destructive updates.

##### Trust Region Optimization

PPO is motivated by trust region methods, which aim to constrain policy updates to maintain stability. While Trust
Region Policy Optimization (TRPO) achieves this through a constraint on the KL divergence between old and new policies,
PPO implements a simpler approach using the clipping mechanism.

The clipping ensures that the ratio $r_t(\theta)$ remains within $[1-\epsilon, 1+\epsilon]$, effectively creating a
trust region that prevents too large policy updates.

##### Policy Update Clipping

<div align="center"> <img src="images/policy-reward-cliff.png" width="600" height="auto"> <p style="color: #555;">Figure: The policy/reward cliff problem</p> </div>

This figure illustrates the "policy/reward cliff" problem that PPO addresses. Without clipping, policy gradient methods
can update policies in ways that lead to catastrophic drops in performance (falling off the cliff). The clipped
surrogate objective prevents this by flattening the optimization landscape in regions where policy changes would be
excessive.

<div align="center"> <img src="images/clipped-surrogate-explained.png" width="600" height="auto"> <p style="color: #555;">Figure: Detailed explanation of the clipped surrogate mechanism</p> </div>

This image breaks down how the clipping mechanism works. The original ratio is shown in red, the clipped ratio in
purple, and the final clipped surrogate (minimum of the two) in blue. For positive advantages, the clipping prevents
excessive increases in action probabilities, while for negative advantages, it prevents excessive decreases.

##### The PPO Algorithm

The full PPO algorithm proceeds as follows:

1. Collect a set of trajectories using the current policy $\pi_{\theta_{\text{old}}}$
2. Compute advantages $A_t$ for each time step
3. Optimize the clipped surrogate objective:
   $$L^{\text{CLIP}}(\theta) = \mathbb{E}_t[\min(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)]$$
4. Update the policy parameters multiple times on the same data (typically 3-10 epochs)
5. Set $\theta_{\text{old}} \leftarrow \theta$ and repeat from step 1

Key advantages of PPO include:

- Simpler implementation than TRPO
- Better sample efficiency than REINFORCE
- More stable learning
- Compatibility with recurrent policies and various neural network architectures
- State-of-the-art performance across many benchmark environments

#### 2. Actor-Critic Methods

Actor-Critic methods represent a hybrid approach that combines policy-based and value-based learning. By simultaneously
learning a policy (actor) and a value function (critic), these methods achieve better sample efficiency and reduce
gradient variance.

##### Actor-Critic Framework

The actor-critic architecture consists of two components:

1. **Actor**: The policy network $\pi_\theta(a|s)$ that selects actions
2. **Critic**: The value network $V_\phi(s)$ that evaluates states or state-action pairs

The actor learns to select actions that maximize expected return, while the critic provides feedback to the actor
through value estimates, reducing the variance of policy gradient updates.

<div align="center"> <img src="images/A2C.png" width="600" height="auto"> <p style="color: #555;">Figure: A2C architecture with synchronization point</p> </div>

This figure shows the Advantage Actor-Critic (A2C) architecture where multiple agents collect experiences in parallel,
and their gradients are synchronized at a central point before updating the shared networks.

##### Advantage Estimation

Instead of using raw returns, actor-critic methods use advantage estimates to update the policy:

$$A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$$

The advantage function tells us how much better taking action $a_t$ in state $s_t$ is compared to the average action
according to the current policy. This provides more precise feedback for policy improvement.

In practice, we often use temporal difference (TD) error as a one-step advantage estimate:

$$A(s_t, a_t) \approx r_t + \gamma V(s_{t+1}) - V(s_t)$$

##### Bias-Variance Trade-off

Actor-critic methods strike a balance in the bias-variance trade-off of policy gradient estimates:

<div align="center"> <img src="images/GAE.png" width="600" height="auto"> <p style="color: #555;">Figure: Generalized Advantage Estimation balancing bias and variance</p> </div>

This image illustrates the Generalized Advantage Estimation (GAE) approach, which combines advantages at different time
scales through a weighted average controlled by the parameter $\lambda$. This provides a flexible mechanism to balance
bias and variance in advantage estimation.

The GAE formula is:

$$A^{\text{GAE}}(s_t, a_t) = \sum_{k=0}^{\infty} (\gamma\lambda)^k \delta_{t+k}$$

Where $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ is the TD error.

##### Monte Carlo vs TD Learning

Actor-critic methods allow a spectrum between Monte Carlo (high variance, no bias) and TD learning (low variance, some
bias):

<div align="center"> <img src="images/bootstrapping.png" width="600" height="auto"> <p style="color: #555;">Figure: Spectrum from TD (one-step) to Monte Carlo (infinite-step) estimation</p> </div>

This figure shows the spectrum from one-step TD (high bias, low variance) to Monte Carlo (no bias, high variance), with
n-step methods in between. The 位 parameter in GAE controls where on this spectrum the advantage estimation falls.

The bias-variance trade-off is fundamental to reinforcement learning:

- Monte Carlo methods use complete returns, resulting in unbiased but high-variance estimates
- TD methods bootstrap from value estimates, resulting in lower variance but biased estimates
- n-step methods and GAE provide a flexible compromise between these extremes

#### 3. Asynchronous Advantage Actor-Critic (A3C)

A3C extends the actor-critic framework by introducing parallel, asynchronous learning across multiple agent instances.
This approach improves both learning efficiency and stability.

##### Parallel Learning

<div align="center"> <img src="images/A2C.png" width="600" height="auto"> <p style="color: #555;">Figure: Parallel actor-critic agents with central parameter synchronization</p> </div>

A3C uses multiple workers (agents), each interacting with its own copy of the environment. These workers collect
experiences independently and compute gradients asynchronously. The gradients are then used to update a global network,
and the updated parameters are periodically synchronized back to the workers.

This parallel architecture offers several benefits:

- Better exploration through diverse experiences
- More stable gradient estimates
- Improved sample efficiency
- Reduced correlation between samples

##### On-policy vs Off-policy Learning

A3C is primarily an on-policy algorithm, meaning it learns from actions taken according to the current policy. In
on-policy learning:

- The same policy is used for both action selection and learning
- No importance sampling correction is needed
- Updates are typically more stable but less sample-efficient

This contrasts with off-policy methods where:

- Different policies are used for action selection and learning
- Importance sampling corrections are needed
- Experience replay buffers can be used for better sample efficiency

The on-policy nature of A3C means that experiences cannot be reused after policy updates, necessitating the continuous
collection of new trajectories.

##### A2C: Synchronous Alternative

A2C (Advantage Actor-Critic) is a synchronous variant of A3C that collects experiences in parallel but synchronizes
updates:

```python
def A2C_update():
    # Collect experiences in parallel
    states, actions, rewards, next_states, dones = collect_experiences(env_vectorized)

    # Compute advantages
    values = critic(states)
    next_values = critic(next_states)
    advantages = rewards + gamma * next_values * (1 - dones) - values

    # Update critic
    value_loss = F.mse_loss(values, rewards + gamma * next_values * (1 - dones))

    # Update actor
    log_probs = actor.log_probs(states, actions)
    policy_loss = -(log_probs * advantages).mean()

    # Add entropy bonus for exploration
    entropy = actor.entropy(states).mean()
    loss = policy_loss + value_coef * value_loss - entropy_coef * entropy

    # Update networks
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

The key differences between A2C and A3C:

- A2C waits for all workers to finish before updating
- A2C typically achieves better GPU utilization
- A2C has more stable updates but potentially higher latency
- A2C is often simpler to implement and debug

##### Generalized Advantage Estimation (GAE)

GAE provides a flexible framework for advantage estimation that significantly improves the performance of actor-critic
methods:

$$A^{\text{GAE}}(s_t, a_t) = \sum_{k=0}^{\infty} (\gamma\lambda)^k \delta_{t+k}$$

Where:

- $\gamma$ is the discount factor for rewards
- $\lambda$ controls the bias-variance trade-off
- $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ is the TD error

<div align="center"> <img src="images/bootstrapping.png" width="600" height="auto"> <p style="color: #555;">Figure: n-step bootstrapping approaches in advantage estimation</p> </div>

The image shows how different values of 位 in GAE correspond to different n-step returns. When 位=0, GAE reduces to the
1-step TD error. When 位=1, GAE becomes equivalent to the Monte Carlo return. Values in between blend multiple n-step
returns for a balanced advantage estimate.

GAE is particularly effective because:

- It reduces variance while controlling bias
- It provides better credit assignment across time steps
- It improves stability in policy optimization
- It's compatible with both A2C and A3C frameworks
